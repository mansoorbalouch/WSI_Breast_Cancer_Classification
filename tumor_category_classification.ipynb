{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "main_dir = \"/media/ist/Drive2/MANSOOR/Neuroimaging-Project/Breast_Cancer_Classification_Project\"\n",
    "pretrained_model_dir = f\"{main_dir}/WSI_Breast_Cancer_Classification/breast_cancer_detection\"\n",
    "model_path_dir = f'{pretrained_model_dir}/saved/models/BCDensenet/0224_034642/'\n",
    "model_config = f\"{model_path_dir}/config.json\"\n",
    "model_path = f\"{model_path_dir}/model_best.pth\"\n",
    "\n",
    "# Ensure the directory containing custom modules is in the path\n",
    "sys.path.append(f'{main_dir}/WSI_Breast_Cancer_Classification/breast_cancer_detection')\n",
    "\n",
    "# Assuming 'parse_config' and other necessary modules are in this directory\n",
    "from breast_cancer_detection import parse_config  # Adjust the import according to actual usage\n",
    "\n",
    "config = parse_config(model_config)  # Adjust if there's a configuration file\n",
    "\n",
    "\n",
    "def load_pretrained_model(model_path):\n",
    "    # Load a custom PyTorch model with necessary configurations\n",
    "    model = CustomModel()\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Define a function to preprocess images\n",
    "def preprocess_image(image_path):\n",
    "    # Assuming the model expects images to be 224x224 pixels\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize the image to fit the model input\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalization\n",
    "    ])\n",
    "    image = Image.open(image_path)\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    return image\n",
    "\n",
    "# Function to predict labels\n",
    "def predict(model, image_path):\n",
    "    image = preprocess_image(image_path)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    return predicted.item()\n",
    "\n",
    "\n",
    "tiles_dir =  f\"{main_dir}/test_tiles\" \n",
    "save_model_path = f\"{main_dir}/WSI_Breast_Cancer_Classification/Model_Weights\"\n",
    "\n",
    "\n",
    "# Load the model\n",
    "model = load_pretrained_model(model_path)\n",
    "\n",
    "# Directory containing images\n",
    "image_directory =  f\"{main_dir}/test_tiles/p_2_test\" \n",
    "image_files = [os.path.join(image_directory, img) for img in os.listdir(image_directory) if img.endswith('.png')]\n",
    "\n",
    "# Predict labels for each image\n",
    "predictions = {img: predict(model, img) for img in image_files}\n",
    "\n",
    "# Print the predictions\n",
    "for img_path, label in predictions.items():\n",
    "    print(f'Image: {img_path}, Predicted Label: {\"Benign\" if label == 0 else \"Malignant\"}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### does not have model weights available #########\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from TransMIL import * \n",
    "from TransMIL.models import TransMIL\n",
    "from TransMIL import MyOptimizer, MyLoss\n",
    "from TransMIL.MyOptimizer import *\n",
    "\n",
    "\n",
    "# Assume you have a function to load the model and it's been trained or a pre-trained model is loaded\n",
    "model = TransMIL()\n",
    "model.load_state_dict(torch.load('trans_mil_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize if necessary\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def classify_patch(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    return predicted.item()\n",
    "\n",
    "\n",
    "# create file directory structure and train-test split (one time run when the file structure is not created)\n",
    "main_dir = \"/media/ist/Drive2/MANSOOR/Neuroimaging-Project/Breast_Cancer_Classification_Project\"\n",
    "\n",
    "tiles_dir =  f\"{main_dir}/test_tiles\" \n",
    "save_model_path = f\"{main_dir}/WSI_Breast_Cancer_Classification/Model_Weights\"\n",
    "\n",
    "# Example usage\n",
    "image_path = f'{tiles_dir}/n_14_test/SUB_n_14_tile_(6_38).png'\n",
    "prediction = classify_patch(image_path)\n",
    "print(\"Predicted class:\", \"Benign\" if prediction == 0 else \"Malignant\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
